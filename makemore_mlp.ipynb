{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data set\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print('Input:')\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n"
     ]
    }
   ],
   "source": [
    "#build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      ".... ---> e\n",
      "...e ---> m\n",
      "..em ---> m\n",
      ".emm ---> a\n",
      "emma ---> .\n",
      "olivia\n",
      ".... ---> o\n",
      "...o ---> l\n",
      "..ol ---> i\n",
      ".oli ---> v\n",
      "oliv ---> i\n",
      "livi ---> a\n",
      "ivia ---> .\n",
      "ava\n",
      ".... ---> a\n",
      "...a ---> v\n",
      "..av ---> a\n",
      ".ava ---> .\n",
      "isabella\n",
      ".... ---> i\n",
      "...i ---> s\n",
      "..is ---> a\n",
      ".isa ---> b\n",
      "isab ---> e\n",
      "sabe ---> l\n",
      "abel ---> l\n",
      "bell ---> a\n",
      "ella ---> .\n",
      "sophia\n",
      ".... ---> s\n",
      "...s ---> o\n",
      "..so ---> p\n",
      ".sop ---> h\n",
      "soph ---> i\n",
      "ophi ---> a\n",
      "phia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 4 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], [] # X is input to neural net. Y is label for each input\n",
    "for w in words[:5]:\n",
    "    \n",
    "    print(w)\n",
    "    context = [0] * block_size # create padded context of Zero tokens\n",
    "    for ch in w + '.': # concat . token to end of word and iterate through charcters\n",
    "        ix = stoi[ch] # assign index of ch to variable ix\n",
    "        X.append(context) # append context to input X\n",
    "        Y.append(ix) # append index of ch to labels Y\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8608, 0.2919])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8608, 0.2919])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing into the column is equivalent to taking the dot product of our lookup table C, with the one hot encoded values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch indexing is very powerful. We can index with a list, or even with a multi-dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.6082e-01,  2.9192e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.6082e-01,  2.9192e-01],\n",
       "         [ 7.4220e-01,  2.7630e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.6082e-01,  2.9192e-01],\n",
       "         [ 7.4220e-01,  2.7630e-01],\n",
       "         [ 7.4220e-01,  2.7630e-01]],\n",
       "\n",
       "        [[ 8.6082e-01,  2.9192e-01],\n",
       "         [ 7.4220e-01,  2.7630e-01],\n",
       "         [ 7.4220e-01,  2.7630e-01],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-1.7604e+00,  1.5907e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-1.7604e+00,  1.5907e+00],\n",
       "         [ 1.8526e+00,  1.4992e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-1.7604e+00,  1.5907e+00],\n",
       "         [ 1.8526e+00,  1.4992e+00],\n",
       "         [-2.8068e-01, -1.1683e+00]],\n",
       "\n",
       "        [[-1.7604e+00,  1.5907e+00],\n",
       "         [ 1.8526e+00,  1.4992e+00],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [ 2.7620e+00,  2.5295e-01]],\n",
       "\n",
       "        [[ 1.8526e+00,  1.4992e+00],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [ 2.7620e+00,  2.5295e-01],\n",
       "         [-2.8068e-01, -1.1683e+00]],\n",
       "\n",
       "        [[-2.8068e-01, -1.1683e+00],\n",
       "         [ 2.7620e+00,  2.5295e-01],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-9.0564e-04,  1.4911e+00],\n",
       "         [ 2.7620e+00,  2.5295e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-9.0564e-04,  1.4911e+00],\n",
       "         [ 2.7620e+00,  2.5295e-01],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-2.8068e-01, -1.1683e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [ 8.4607e-01, -3.9618e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [ 8.4607e-01, -3.9618e-01],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-2.8068e-01, -1.1683e+00],\n",
       "         [ 8.4607e-01, -3.9618e-01],\n",
       "         [-9.0564e-04,  1.4911e+00],\n",
       "         [ 1.3411e-02,  2.0805e+00]],\n",
       "\n",
       "        [[ 8.4607e-01, -3.9618e-01],\n",
       "         [-9.0564e-04,  1.4911e+00],\n",
       "         [ 1.3411e-02,  2.0805e+00],\n",
       "         [ 8.6082e-01,  2.9192e-01]],\n",
       "\n",
       "        [[-9.0564e-04,  1.4911e+00],\n",
       "         [ 1.3411e-02,  2.0805e+00],\n",
       "         [ 8.6082e-01,  2.9192e-01],\n",
       "         [ 1.8526e+00,  1.4992e+00]],\n",
       "\n",
       "        [[ 1.3411e-02,  2.0805e+00],\n",
       "         [ 8.6082e-01,  2.9192e-01],\n",
       "         [ 1.8526e+00,  1.4992e+00],\n",
       "         [ 1.8526e+00,  1.4992e+00]],\n",
       "\n",
       "        [[ 8.6082e-01,  2.9192e-01],\n",
       "         [ 1.8526e+00,  1.4992e+00],\n",
       "         [ 1.8526e+00,  1.4992e+00],\n",
       "         [-9.0564e-04,  1.4911e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.4607e-01, -3.9618e-01]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.4607e-01, -3.9618e-01],\n",
       "         [-1.7604e+00,  1.5907e+00]],\n",
       "\n",
       "        [[-4.3063e-01, -2.0792e-01],\n",
       "         [ 8.4607e-01, -3.9618e-01],\n",
       "         [-1.7604e+00,  1.5907e+00],\n",
       "         [ 8.6845e-01, -5.6675e-01]],\n",
       "\n",
       "        [[ 8.4607e-01, -3.9618e-01],\n",
       "         [-1.7604e+00,  1.5907e+00],\n",
       "         [ 8.6845e-01, -5.6675e-01],\n",
       "         [ 1.1872e-01, -1.0339e+00]],\n",
       "\n",
       "        [[-1.7604e+00,  1.5907e+00],\n",
       "         [ 8.6845e-01, -5.6675e-01],\n",
       "         [ 1.1872e-01, -1.0339e+00],\n",
       "         [-2.8068e-01, -1.1683e+00]],\n",
       "\n",
       "        [[ 8.6845e-01, -5.6675e-01],\n",
       "         [ 1.1872e-01, -1.0339e+00],\n",
       "         [-2.8068e-01, -1.1683e+00],\n",
       "         [-9.0564e-04,  1.4911e+00]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4306, -0.2079])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.0564e-04,  1.4911e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our embedding\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our weights and biases\n",
    "# Weights are 6x100. 6 because our output from embedding is 3x2=6, so there are 6 inputs to this hidden layer. 100 neurons has been chosen randomly.\n",
    "# We also instantiate 100 biases to match our 100 weights\n",
    "W1 = torch.randn((8, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01,  8.6082e-01,\n",
       "          2.9192e-01],\n",
       "        [-4.3063e-01, -2.0792e-01,  8.6082e-01,  2.9192e-01,  7.4220e-01,\n",
       "          2.7630e-01],\n",
       "        [ 8.6082e-01,  2.9192e-01,  7.4220e-01,  2.7630e-01,  7.4220e-01,\n",
       "          2.7630e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -1.7604e+00,\n",
       "          1.5907e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -1.7604e+00,  1.5907e+00,  1.8526e+00,\n",
       "          1.4992e+00],\n",
       "        [-1.7604e+00,  1.5907e+00,  1.8526e+00,  1.4992e+00, -2.8068e-01,\n",
       "         -1.1683e+00],\n",
       "        [ 1.8526e+00,  1.4992e+00, -2.8068e-01, -1.1683e+00,  2.7620e+00,\n",
       "          2.5295e-01],\n",
       "        [-2.8068e-01, -1.1683e+00,  2.7620e+00,  2.5295e-01, -2.8068e-01,\n",
       "         -1.1683e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -9.0564e-04,\n",
       "          1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -9.0564e-04,  1.4911e+00,  2.7620e+00,\n",
       "          2.5295e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -2.8068e-01,\n",
       "         -1.1683e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -2.8068e-01, -1.1683e+00,  8.4607e-01,\n",
       "         -3.9618e-01],\n",
       "        [-2.8068e-01, -1.1683e+00,  8.4607e-01, -3.9618e-01, -9.0564e-04,\n",
       "          1.4911e+00],\n",
       "        [ 8.4607e-01, -3.9618e-01, -9.0564e-04,  1.4911e+00,  1.3411e-02,\n",
       "          2.0805e+00],\n",
       "        [-9.0564e-04,  1.4911e+00,  1.3411e-02,  2.0805e+00,  8.6082e-01,\n",
       "          2.9192e-01],\n",
       "        [ 1.3411e-02,  2.0805e+00,  8.6082e-01,  2.9192e-01,  1.8526e+00,\n",
       "          1.4992e+00],\n",
       "        [ 8.6082e-01,  2.9192e-01,  1.8526e+00,  1.4992e+00,  1.8526e+00,\n",
       "          1.4992e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01,  8.4607e-01,\n",
       "         -3.9618e-01],\n",
       "        [-4.3063e-01, -2.0792e-01,  8.4607e-01, -3.9618e-01, -1.7604e+00,\n",
       "          1.5907e+00],\n",
       "        [ 8.4607e-01, -3.9618e-01, -1.7604e+00,  1.5907e+00,  8.6845e-01,\n",
       "         -5.6675e-01],\n",
       "        [-1.7604e+00,  1.5907e+00,  8.6845e-01, -5.6675e-01,  1.1872e-01,\n",
       "         -1.0339e+00],\n",
       "        [ 8.6845e-01, -5.6675e-01,  1.1872e-01, -1.0339e+00, -2.8068e-01,\n",
       "         -1.1683e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat concats tensors. takes a positional argument for which dimension to concat on. cat is an inefficient operation\n",
    "# this works, but if we change our block size this would break and we'd need to add more arguments to concat. lets try torch.unbind\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -4.3063e-01, -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01,  8.6082e-01,  2.9192e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01,  8.6082e-01,\n",
       "          2.9192e-01,  7.4220e-01,  2.7630e-01],\n",
       "        [-4.3063e-01, -2.0792e-01,  8.6082e-01,  2.9192e-01,  7.4220e-01,\n",
       "          2.7630e-01,  7.4220e-01,  2.7630e-01],\n",
       "        [ 8.6082e-01,  2.9192e-01,  7.4220e-01,  2.7630e-01,  7.4220e-01,\n",
       "          2.7630e-01, -9.0564e-04,  1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -4.3063e-01, -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -1.7604e+00,  1.5907e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -1.7604e+00,\n",
       "          1.5907e+00,  1.8526e+00,  1.4992e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -1.7604e+00,  1.5907e+00,  1.8526e+00,\n",
       "          1.4992e+00, -2.8068e-01, -1.1683e+00],\n",
       "        [-1.7604e+00,  1.5907e+00,  1.8526e+00,  1.4992e+00, -2.8068e-01,\n",
       "         -1.1683e+00,  2.7620e+00,  2.5295e-01],\n",
       "        [ 1.8526e+00,  1.4992e+00, -2.8068e-01, -1.1683e+00,  2.7620e+00,\n",
       "          2.5295e-01, -2.8068e-01, -1.1683e+00],\n",
       "        [-2.8068e-01, -1.1683e+00,  2.7620e+00,  2.5295e-01, -2.8068e-01,\n",
       "         -1.1683e+00, -9.0564e-04,  1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -4.3063e-01, -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -9.0564e-04,  1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -9.0564e-04,\n",
       "          1.4911e+00,  2.7620e+00,  2.5295e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -9.0564e-04,  1.4911e+00,  2.7620e+00,\n",
       "          2.5295e-01, -9.0564e-04,  1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -4.3063e-01, -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -2.8068e-01, -1.1683e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -2.8068e-01,\n",
       "         -1.1683e+00,  8.4607e-01, -3.9618e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -2.8068e-01, -1.1683e+00,  8.4607e-01,\n",
       "         -3.9618e-01, -9.0564e-04,  1.4911e+00],\n",
       "        [-2.8068e-01, -1.1683e+00,  8.4607e-01, -3.9618e-01, -9.0564e-04,\n",
       "          1.4911e+00,  1.3411e-02,  2.0805e+00],\n",
       "        [ 8.4607e-01, -3.9618e-01, -9.0564e-04,  1.4911e+00,  1.3411e-02,\n",
       "          2.0805e+00,  8.6082e-01,  2.9192e-01],\n",
       "        [-9.0564e-04,  1.4911e+00,  1.3411e-02,  2.0805e+00,  8.6082e-01,\n",
       "          2.9192e-01,  1.8526e+00,  1.4992e+00],\n",
       "        [ 1.3411e-02,  2.0805e+00,  8.6082e-01,  2.9192e-01,  1.8526e+00,\n",
       "          1.4992e+00,  1.8526e+00,  1.4992e+00],\n",
       "        [ 8.6082e-01,  2.9192e-01,  1.8526e+00,  1.4992e+00,  1.8526e+00,\n",
       "          1.4992e+00, -9.0564e-04,  1.4911e+00],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01, -4.3063e-01, -2.0792e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01, -4.3063e-01,\n",
       "         -2.0792e-01,  8.4607e-01, -3.9618e-01],\n",
       "        [-4.3063e-01, -2.0792e-01, -4.3063e-01, -2.0792e-01,  8.4607e-01,\n",
       "         -3.9618e-01, -1.7604e+00,  1.5907e+00],\n",
       "        [-4.3063e-01, -2.0792e-01,  8.4607e-01, -3.9618e-01, -1.7604e+00,\n",
       "          1.5907e+00,  8.6845e-01, -5.6675e-01],\n",
       "        [ 8.4607e-01, -3.9618e-01, -1.7604e+00,  1.5907e+00,  8.6845e-01,\n",
       "         -5.6675e-01,  1.1872e-01, -1.0339e+00],\n",
       "        [-1.7604e+00,  1.5907e+00,  8.6845e-01, -5.6675e-01,  1.1872e-01,\n",
       "         -1.0339e+00, -2.8068e-01, -1.1683e+00],\n",
       "        [ 8.6845e-01, -5.6675e-01,  1.1872e-01, -1.0339e+00, -2.8068e-01,\n",
       "         -1.1683e+00, -9.0564e-04,  1.4911e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.unbind removes a tensor dimension. it returns a tuple of all slices along this dimension, without it. this method is insensitive to changes in block size\n",
    "torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_14280\\650436493.py:4: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  display(a.storage()) # tensors are stored in memory as a list of their values\n",
      "c:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py:700: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  output = repr(obj)\n",
      "c:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\torch\\storage.py:645: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return str(self)\n",
      "c:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\torch\\storage.py:636: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f'device={self.device}) of size {len(self)}]')\n",
      "c:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\torch\\storage.py:637: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type == 'meta':\n",
      "c:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\torch\\storage.py:640: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  data_str = ' ' + '\\n '.join(str(self[i]) for i in range(self.size()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12, 13, 14, 15, 16, 17]]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12, 13, 14, 15, 16, 17]]),\n",
       " tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5]],\n",
       " \n",
       "         [[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]],\n",
       " \n",
       "         [[12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17]]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor.view() is an extremely efficient way to maniuplate tensor dimensions without actually modifying memory. \n",
    "# It utilizes underlying attributes of the tensor object in order to change the way a tensor is represented.\n",
    "a = torch.arange(18)\n",
    "display(a.storage()) # tensors are stored in memory as a list of their values\n",
    "a.view(2,9), a.view(2,9), a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.view() just so happens to manipulate the shape of the tensor in the way we desire\n",
    "# Using -1 to avoid hardcoding. We could equivalently use emb.shape[0]. PyTorch is smart enough to derive the necessary value to make the operation work using the second value of 6\n",
    "# We use tanh here to transfrom all values in the resulting tesnor to be between -1 and 1\n",
    "# **It's good practice to always double check that the broadcasting operation taking place (+) works as we expect it to!\n",
    "h = torch.tanh(emb.view(-1, 8) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.6206)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative log loss likelihood\n",
    "loss = -prob[torch.arange(len(Y)), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the above code rewritten and condensed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], [] # X is input to neural net. Y is label for each input\n",
    "for w in words:\n",
    "    \n",
    "    context = [0] * block_size # create padded context of Zero tokens\n",
    "    for ch in w + '.': # concat . token to end of word and iterate through charcters\n",
    "        ix = stoi[ch] # assign index of ch to variable ix\n",
    "        X.append(context) # append context to input X\n",
    "        Y.append(ix) # append index of ch to labels Y\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dataset with train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182441, 3]) torch.Size([182441])\n",
      "torch.Size([22902, 3]) torch.Size([22902])\n",
      "torch.Size([22803, 3]) torch.Size([22803])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "    X, Y = [], [] # X is input to neural net. Y is label for each input\n",
    "    for w in words:\n",
    "        \n",
    "        context = [0] * block_size # create padded context of Zero tokens\n",
    "        for ch in w + '.': # concat . token to end of word and iterate through charcters\n",
    "            ix = stoi[ch] # assign index of ch to variable ix\n",
    "            X.append(context) # append context to input X\n",
    "            Y.append(ix) # append index of ch to labels Y\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # F.cross_entropy()\n",
    "# emb = C[X] # (32, 3, 2)\n",
    "# h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "# logits = h @ W2 + b2 # (32, 27)\n",
    "# # counts = logits.exp()\n",
    "# # prob = counts / counts.sum(1, keepdims=True)\n",
    "# # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "\n",
    "# # ^^^ THIS IS THE SAME AS THE ABOVE ^^^\n",
    "# # F.cross_entropy is much more computationally efficient as it does compound operations rather than multiple individual operations.\n",
    "# # In addition, it makes back-propagation much more efficient for the same reasons\n",
    "# loss = F.cross_entropy(logits, Y)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "block_size = 3\n",
    "n_hidden = 200\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3)/((n_embd* block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                      generator=g) * 0.02\n",
    "W2 = torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias] # to count total parameters\n",
    "print(f'Total parameters: {sum(p.nelement() for p in parameters)}')\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_floor(ix, T, buff):\n",
    "    return T[:,ix].min().item() - (T[:,ix].max().item() - T[:,ix].min().item()) * buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['b' if i not in {1, 5, 9, 15, 21} else 'crimson' for i in np.arange(27)]\n",
    "\n",
    "# def save_fig(frame):\n",
    "#     plt.figure(figsize=(8,8))\n",
    "#     plt.scatter(C[:,0].data, C[:,1].data, s=200, c=colors)\n",
    "#     for i in range(C.shape[0]):\n",
    "#         plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "#     # plt.grid('minor')\n",
    "#     plt.savefig(f'GIF/{frame}.png', dpi=500)\n",
    "#     plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b' if i not in {1, 5, 9, 15, 21} else 'crimson' for i in np.arange(27)]\n",
    "\n",
    "def save_fig_3d(frame):\n",
    "    n_chars = 5\n",
    "    frame_s = str(frame)\n",
    "    n = n_chars - len(frame_s)\n",
    "    frame_s = ('0' * n) + frame_s\n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = plt.axes(projection='3d') \n",
    "    # plot shadows\n",
    "    floor = get_floor(2, C, 0.1)\n",
    "    ax.scatter3D(C[:,0].data, C[:,1].data, floor, c='gray', s=150, zorder=2)\n",
    "    # plot points\n",
    "    ax.scatter3D(C[:,0].data, C[:,1].data, C[:,2].data, c=colors, s=150, zorder=20)\n",
    "\n",
    "    # plot labels\n",
    "    for i in range(C.shape[0]):\n",
    "        ax.text3D(C[i,0].item(), C[i,1].item(), C[i,2].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\", zorder=40, fontsize=8)\n",
    "    for i in range(C.shape[0]):\n",
    "        ax.text3D(C[i,0].item(), C[i,1].item(), floor, itos[i], ha=\"center\", va=\"center\", color=\"white\", zorder=10, fontsize=8)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title('Training of Embeddings Over 1M Iterations')\n",
    "    plt.savefig('GIF_3d/' + frame_s)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig_2d(frame):\n",
    "    n_chars = 5\n",
    "    frame_s = str(frame)\n",
    "    n = n_chars - len(frame_s)\n",
    "    frame_s = ('0' * n) + frame_s\n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = plt.axes() \n",
    "    # plot points\n",
    "    ax.scatter(C[:,0].data, C[:,1].data, c=colors, s=200)\n",
    "\n",
    "    # plot labels\n",
    "    for i in range(C.shape[0]):\n",
    "        ax.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Training of Embeddings Over 50k Iterations')\n",
    "    plt.savefig('GIF_2d/' + frame_s)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200000:\n",
      "batch loss: 3.281588\n",
      "\n",
      "10000/200000:\n",
      "batch loss: 2.170333\n",
      "\n",
      "20000/200000:\n",
      "batch loss: 2.096152\n",
      "\n",
      "30000/200000:\n",
      "batch loss: 1.914917\n",
      "\n",
      "40000/200000:\n",
      "batch loss: 2.107869\n",
      "\n",
      "50000/200000:\n",
      "batch loss: 2.082729\n",
      "\n",
      "60000/200000:\n",
      "batch loss: 2.131227\n",
      "\n",
      "70000/200000:\n",
      "batch loss: 1.801952\n",
      "\n",
      "80000/200000:\n",
      "batch loss: 2.012656\n",
      "\n",
      "90000/200000:\n",
      "batch loss: 2.009828\n",
      "\n",
      "100000/200000:\n",
      "batch loss: 2.010406\n",
      "\n",
      "110000/200000:\n",
      "batch loss: 1.979393\n",
      "\n",
      "120000/200000:\n",
      "batch loss: 1.823938\n",
      "\n",
      "130000/200000:\n",
      "batch loss: 2.179720\n",
      "\n",
      "140000/200000:\n",
      "batch loss: 1.816852\n",
      "\n",
      "150000/200000:\n",
      "batch loss: 2.263287\n",
      "\n",
      "160000/200000:\n",
      "batch loss: 2.135348\n",
      "\n",
      "170000/200000:\n",
      "batch loss: 2.111442\n",
      "\n",
      "180000/200000:\n",
      "batch loss: 2.071927\n",
      "\n",
      "190000/200000:\n",
      "batch loss: 2.075921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 64\n",
    "frames = 1000\n",
    "frame_n = 0\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # construct minibatch\n",
    "    # create a tensor ix of length batch_size and fill it\n",
    "    # with values within range 0 to Xtr.shape[0]\n",
    "    # use ix index into Xtr, Ytr, assign output to Xb, Yb\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact = embcat @ W1 #+ b1 # hidden layer preactivation\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / (bnstdi) + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = (0.999 * bnmean_running) + (0.001 * bnmeani)\n",
    "        bnstd_running = (0.999 * bnstd_running) + (0.001 * bnstdi)\n",
    "\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # calculate loss\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:                                           \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % (max_steps / 20) == 0:\n",
    "        print(f'{i}/{max_steps}:')\n",
    "        print(f'batch loss: {loss.item():.6f}\\n')\n",
    "    \n",
    "\n",
    "    # if (i % 50) == 0 and i < 50000:\n",
    "    #     save_fig_2d(int(frame_n))\n",
    "    #     frame_n += 1\n",
    "    # elif i == (max_steps-1):\n",
    "    #     save_fig_2d(frame_n)\n",
    "    #     frame_n += 1\n",
    "\n",
    "\n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x] # (32, 3, 2)\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split}: {loss.item()}')\n",
    "\n",
    "# split_loss('train')\n",
    "# split_loss('val')\n",
    "# split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2.0428762435913086\n",
      "val: 2.1018285751342773\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's sample from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[x] # (32, 3, 2)\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "hpreact = embcat @ W1 + b1\n",
    "hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / (hpreact.std(0, keepdim=True)) + bnbias\n",
    "h = torch.tanh(hpreact) # (32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alina\n",
      "harley\n",
      "aolus\n",
      "padis\n",
      "siden\n",
      "klohanise\n",
      "jere\n",
      "jayna\n",
      "aley\n",
      "hanti\n",
      "phoy\n",
      "kinzlynn\n",
      "sara\n",
      "yask\n",
      "bra\n",
      "massa\n",
      "santae\n",
      "wrentlee\n",
      "aune\n",
      "katana\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 2023) # for reproducibility\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all '...'\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1, block_size) tensor with current context\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "        h = torch.tanh(hpreact) # pass current context into model\n",
    "        logits = h @ W2 + b2 # retrieve output logits from hidden layer\n",
    "        probs = F.softmax(logits, dim=1) # converts logits to probabilities\n",
    "        # multinomial samples from our probability distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix] # reset our context tensor to 3 chars\n",
    "        if ix == 0:\n",
    "            break # break loop if we've selected a '.' character\n",
    "        out.append(ix) # append selected character to output word\n",
    "    print(''.join(itos[i] for i in out)) # print output word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is to modify hyperparameters in order to improve on the log loss!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
